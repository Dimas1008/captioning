# -*- coding: utf-8 -*-
"""Imagae Captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XzwxTqGSFxhrQYpblfa2oSIWDa_dbVtc
"""

# Import necessary libraries

import zipfile
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Flatten, add
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import os
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from sklearn.preprocessing import LabelEncoder

# from google.colab import drive
# drive.mount('/content/drive')

# !unzip '/content/drive/MyDrive/ulcer/ulcer_diabetes.zip'

"""## **Step 1: Data Understanding**"""

# Load Dataset and understand its structure
file_path_csv = '/content/Patches/diabetes_ulcer_final_varied_dataset.csv'

dataset = pd.read_csv(file_path_csv)
dataset.head()

dataset.info()

"""# **Step 2: Data Preparation**"""

# Extract relevant columns
dataset['image_path'] = dataset['image'].apply(lambda x: os.path.join('/content/Patches/images', x.split(',')[0].strip()))

# Split the dataset into training and validation sets
train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)

# Tokenize captions
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(train_data['caption'].astype(str).fillna(''))
train_sequences = tokenizer.texts_to_sequences(train_data['caption'].astype(str).fillna(''))
train_padded = pad_sequences(train_sequences, padding='post')

val_sequences = tokenizer.texts_to_sequences(val_data['caption'].astype(str).fillna(''))
val_padded = pad_sequences(val_sequences, padding='post')

# Encode captions with LabelEncoder to ensure all values are numeric and within bounds
caption_encoder = LabelEncoder()
train_data['caption_encoded'] = caption_encoder.fit_transform(train_data['caption'].astype(str))
val_data['caption_encoded'] = caption_encoder.transform(val_data['caption'].astype(str))

"""## **Step 3: Train and Fine-tune VGG16 for Classification**"""

# Load VGG16 pre-trained model without the top layer (include_top=False)
vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Unfreeze some VGG16 layers for fine-tuning
for layer in vgg_model.layers:
    layer.trainable = True  # Unfreeze all layers for fine-tuning

# Add custom layers for classification
x = vgg_model.output
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(3, activation='softmax')(x)  # Assuming 3 classes for level classification

# Compile the model
vgg_classification_model = Model(inputs=vgg_model.input, outputs=output)
vgg_classification_model.summary()

vgg_classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

# Prepare image data for training the VGG16 model
train_images = np.array([img_to_array(load_img(img_path, target_size=(224, 224))) for img_path in train_data['image_path']])
train_images = tf.keras.applications.vgg16.preprocess_input(train_images)
val_images = np.array([img_to_array(load_img(img_path, target_size=(224, 224))) for img_path in val_data['image_path']])
val_images = tf.keras.applications.vgg16.preprocess_input(val_images)

# Label encode the categorical column 'level'
encoder = LabelEncoder()
train_data['level'] = encoder.fit_transform(train_data['level'].astype(str))
val_data['level'] = encoder.transform(val_data['level'].astype(str))

train_labels = tf.keras.utils.to_categorical(train_data['level'], num_classes=len(encoder.classes_))
val_labels = tf.keras.utils.to_categorical(val_data['level'], num_classes=len(encoder.classes_))

# Train the VGG16 model
vgg_classification_model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=10, batch_size=32)

# Evaluate the VGG16 model
evaluation_vgg = vgg_classification_model.evaluate(val_images, val_labels)
print(f"VGG16 Evaluation Loss: {evaluation_vgg[0]}")
print(f"VGG16 Evaluation Accuracy: {evaluation_vgg[1]}")

"""## **Step 4: Extract Features Using Trained VGG16 Model**"""

# Extract features for the entire dataset
train_image_features = vgg_classification_model.predict(train_images)
val_image_features = vgg_classification_model.predict(val_images)

train_sequences = tokenizer.texts_to_sequences(train_data['caption'].astype(str).fillna(''))
train_padded = pad_sequences(train_sequences, padding='post')
val_sequences = tokenizer.texts_to_sequences(val_data['caption'].astype(str).fillna(''))
val_padded = pad_sequences(val_sequences, padding='post')

"""## **Step 5: Train LSTM Model for Caption Generation**"""

# Sequential model to process extracted image features followed by LSTM for captions
lstm_model = Sequential()
lstm_model.add(Input(shape=(train_padded.shape[1],)))  # Input for caption sequence
lstm_model.add(Embedding(input_dim=5000, output_dim=256, mask_zero=True))
lstm_model.add(LSTM(256, return_sequences=True))
lstm_model.add(Dropout(0.5))
lstm_model.add(LSTM(256))
lstm_model.add(Dropout(0.5))
lstm_model.add(Dense(256, activation='relu'))
lstm_model.add(Dense(len(caption_encoder.classes_), activation='softmax'))  # 4 outputs for level, infection, recommendation, and caption

# Compile the LSTM model
lstm_model.summary()

lstm_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])

# Train the LSTM model using image features and caption sequences
lstm_model.fit(
    [train_image_features, train_padded],
    tf.keras.utils.to_categorical(train_data['caption_encoded'].values, num_classes=len(caption_encoder.classes_)),
    validation_data=([val_image_features, val_padded], tf.keras.utils.to_categorical(val_data['caption_encoded'].values, num_classes=len(caption_encoder.classes_))),
    epochs=10,
    batch_size=32
)

evaluation_lstm = lstm_model.evaluate(
    [val_image_features, val_padded],
    tf.keras.utils.to_categorical(val_data['caption_encoded'].values, num_classes=len(caption_encoder.classes_))
)
print(f"LSTM Evaluation Loss: {evaluation_lstm[0]}")
print(f"LSTM Evaluation Accuracy: {evaluation_lstm[1]}")

"""## **Step 6: Combine VGG16 and LSTM Models**"""

# Define inputs for the combined model
# Define inputs for the combined model
image_input = Input(shape=(224, 224, 3))
caption_input = Input(shape=(train_padded.shape[1],))

# Use VGG16 for feature extraction from images
vgg_features = vgg_classification_model(image_input)

# Embedding and LSTM for text data
embedding_layer = Embedding(input_dim=5000, output_dim=256, mask_zero=True)(caption_input)
lstm_out = LSTM(256, return_sequences=True)(embedding_layer)
lstm_out = Dropout(0.5)(lstm_out)
lstm_out = LSTM(256)(lstm_out)
lstm_out = Dropout(0.5)(lstm_out)

# Combine image features and LSTM output
from tensorflow.keras.layers import Concatenate

vgg_features_flat = Flatten()(vgg_features)
combined = Concatenate()([vgg_features_flat, lstm_out])
x = Dense(256, activation='relu')(combined)
output = Dense(len(caption_encoder.classes_), activation='softmax')(x)  # 4 outputs for level, infection, recommendation, and caption

# Full combined model
combined_model = Model(inputs=[image_input, caption_input], outputs=output)

# Compile the combined model
combined_model.summary()

combined_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])

# Train the combined model
combined_model.fit(
    [train_images, train_padded],
    tf.keras.utils.to_categorical(train_data['caption_encoded'].values, num_classes=len(caption_encoder.classes_)),
    validation_data=([val_images, val_padded], tf.keras.utils.to_categorical(val_data['caption_encoded'].values, num_classes=len(caption_encoder.classes_))),
    epochs=10,
    batch_size=32
)

# Evaluate the combined model
evaluation_combined = combined_model.evaluate(
    [val_images, val_padded],
    tf.keras.utils.to_categorical(val_data['caption_encoded'].values, num_classes=len(caption_encoder.classes_))
)
print(f"Combined Model Evaluation Loss: {evaluation_combined[0]}")
print(f"Combined Model Evaluation Accuracy: {evaluation_combined[1]}")

"""## **Step 7: Save the Models**"""

# # Save the VGG16 classification model
# vgg_classification_model.save('vgg16_classification_model.h5')
# converter = tf.lite.TFLiteConverter.from_keras_model(vgg_classification_model)
# vgg16_tflite_model = converter.convert()
# with open('vgg16_classification_model.tflite', 'wb') as f:
#     f.write(vgg16_tflite_model)

# # Save the LSTM model
# lstm_model.save('lstm_captioning_model.h5')
# converter = tf.lite.TFLiteConverter.from_keras_model(lstm_model)
# lstm_tflite_model = converter.convert()
# with open('lstm_captioning_model.tflite', 'wb') as f:
#     f.write(lstm_tflite_model)

# # Save the combined model
# combined_model.save('combined_model_savedmodel')
# converter = tf.lite.TFLiteConverter.from_keras_model(combined_model)
# combined_tflite_model = converter.convert()
# with open('combined_model.tflite', 'wb') as f:
#     f.write(combined_tflite_model)

"""## **Step 8: Load and Preprocess Image for Prediction**"""

image_path = '/content/Patches/images/abnormal (1).jpg'  # Replace with the actual image path
processed_image = load_img(image_path, target_size=(224, 224))
processed_image = img_to_array(processed_image)
processed_image = tf.keras.applications.vgg16.preprocess_input(processed_image)

# Dummy caption input for prediction (using padding)
dummy_caption = pad_sequences([[1, 2, 3]], maxlen=train_padded.shape[1], padding='post')

# Generate Prediction using the Combined Model
prediction = combined_model.predict([np.expand_dims(processed_image, axis=0), dummy_caption])

# Display the Image
import matplotlib.pyplot as plt
img = load_img(image_path, target_size=(224, 224))
img_array = img_to_array(img) / 255.0
plt.imshow(img_array)
plt.axis('off')
plt.title('Input Image')
plt.show()

# Print the Results
predicted_class_index = np.argmax(prediction[0])
class_name = caption_encoder.inverse_transform([predicted_class_index])[0]
print(f"Predicted class: {class_name}")
print(f"Raw Predicted values: {prediction[0]}")

image_path = '/content/Patches/images/normal (438).jpg'  # Replace with the actual image path
processed_image = load_img(image_path, target_size=(224, 224))
processed_image = img_to_array(processed_image)
processed_image = tf.keras.applications.vgg16.preprocess_input(processed_image)

# Dummy caption input for prediction (using padding)
dummy_caption = pad_sequences([[1, 2, 3]], maxlen=train_padded.shape[1], padding='post')

# Generate Prediction using the Combined Model
prediction = combined_model.predict([np.expand_dims(processed_image, axis=0), dummy_caption])

# Display the Image
import matplotlib.pyplot as plt
img = load_img(image_path, target_size=(224, 224))
img_array = img_to_array(img) / 255.0
plt.imshow(img_array)
plt.axis('off')
plt.title('Input Image')
plt.show()

# Print the Results
predicted_class_index = np.argmax(prediction[0])
class_name = caption_encoder.inverse_transform([predicted_class_index])[0]
print(f"Predicted class: {class_name}")
print(f"Raw Predicted values: {prediction[0]}")

